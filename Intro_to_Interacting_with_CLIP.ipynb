{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu6IEGvzEe4Wyaqm1SgfeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Diffusers/blob/main/Intro_to_Interacting_with_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/cagBRT/Diffusers.git\n",
        "\n",
        "!git clone -l -s https://github.com/cagBRT/Diffusers.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "2eaMOh2gSYPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "nEHbBAOhI_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSioPdPSI5vT"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# we initialize a tokenizer, image processor, and the model itself\n",
        "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "model = CLIPModel.from_pretrained(model_id).to(device)"
      ],
      "metadata": {
        "id": "GPCodDeeJDpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls=['https://images.unsplash.com/photo-1662955676669-c5d141718bfd?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=687&q=80',\n",
        "    'https://images.unsplash.com/photo-1552053831-71594a27632d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=662&q=80',\n",
        "    'https://images.unsplash.com/photo-1530281700549-e82e7bf110d6?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=688&q=80' ]\n",
        "\n",
        "images=[Image.open(requests.get(i, stream=True).raw)  for i in urls]"
      ],
      "metadata": {
        "id": "1pvR0lx0JGrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images"
      ],
      "metadata": {
        "id": "_t9cpZAfJJLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "fZDyAiiLJLbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]\n"
      ],
      "metadata": {
        "id": "bUmZGhDcJL_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "5Mu4JY3_JMJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts=[\"a girl wearing a beanie\", \"a boy wearing a beanie\", \"a dog\", \"a dog at the beach\"]\n",
        "inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)"
      ],
      "metadata": {
        "id": "rqexANrFO99A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "probs = logits_per_image.softmax(dim=1)"
      ],
      "metadata": {
        "id": "iFntjp3qRudv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(probs.detach().numpy()*100, columns=text_prompts, index=list(['image1','image2', 'image3'])).style.background_gradient(axis=None,low=0, high=0.91).format(precision=2)"
      ],
      "metadata": {
        "id": "ORLODq5iPCdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts=[\"a girl wearing a beanie\", \"a boy wearing a beanie\", \"a dog\", \"a dog at the beach\",\"russian dolls\"]\n",
        "inputs = inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "probs = logits_per_image.softmax(dim=1)"
      ],
      "metadata": {
        "id": "xzDDItM_JRyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a image that the model has not seen before. <br>\n",
        "Can it predict a text description?"
      ],
      "metadata": {
        "id": "eGYmmlDpNqzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#images_new = \"russsian dolls.jpeg\"\n",
        "images_new=\"screaming people.png\""
      ],
      "metadata": {
        "id": "98NMkUtvKfET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images4=Image.open(images_new)"
      ],
      "metadata": {
        "id": "KWTTIke9MWer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images4"
      ],
      "metadata": {
        "id": "mbVdpm85ObTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.append(images4)"
      ],
      "metadata": {
        "id": "0bHkFyLIKphE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image4=images[3]"
      ],
      "metadata": {
        "id": "VobGEaybNPVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts=[\"a girl wearing a beanie\",\"a boy wearing a beanie\", \"a dog\", \"a dog at the beach\",\"russian dolls\"]#, \"herd of cats\",\"screaming people\"]\n",
        "inputs = inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "probs = logits_per_image.softmax(dim=1)"
      ],
      "metadata": {
        "id": "MU5j__OANfGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(probs.detach().numpy()*100, columns=text_prompts, index=list(['image1','image2', 'image3','image4'])).style.background_gradient(axis=None,low=0, high=0.91).format(precision=2)"
      ],
      "metadata": {
        "id": "68uKHwDjJUzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment<br>\n",
        "Find another image, see how the model does with an image it has never seen before. <br>\n",
        "Try different prompts, how does the model do when the prompts are similar?(girl with beanie, woman in winter gear, girl in winter wear)"
      ],
      "metadata": {
        "id": "doQOJAt6bKxU"
      }
    }
  ]
}